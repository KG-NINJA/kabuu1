name: Stock Price Prediction Pipeline
run: |
    flake8 src/ --max-line-length=100 --exclude=reinforcement_learning.py --count --statistics || true

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main
  schedule:
    # 毎日UTC 09:00に予測実行（営業時間前）
    - cron: '0 9 * * 1-5'
    # 毎週月曜UTC 08:00にモデル再学習
    - cron: '0 8 * * 1'
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  DATA_DIR: 'data/stock_data'
  MODEL_DIR: 'models'

jobs:
  # ========== ビルド & テスト ==========
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt

      - name: Run unit tests
        run: |
          pytest tests/ -v --tb=short --cov=src --cov-report=xml
          
      - name: Run code quality checks
        run: |
          flake8 src/ --max-line-length=100 --exclude=reinforcement_learning.py
          black --check src/ --exclude=reinforcement_learning.py
          mypy src/ --ignore-missing-imports --exclude=reinforcement_learning.py

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml

  # ========== データ取得・前処理 ==========
  fetch-data:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: build
    if: github.event_name == 'schedule' || github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Create data directories
        run: |
          mkdir -p ${{ env.DATA_DIR }}
          mkdir -p ${{ env.MODEL_DIR }}
          mkdir -p logs

      - name: Fetch stock data
        run: |
          python src/data_fetcher.py \
            --us-symbols AAPL GOOGL MSFT TSLA \
            --jp-symbols 9984 6758 7203 8306 \
            --output ${{ env.DATA_DIR }}/raw_data.csv \
            --log logs/fetch.log
          echo "✅ Stock data fetched (US + JP)"

      - name: Data validation
        run: |
          python src/data_validator.py \
            --input ${{ env.DATA_DIR }}/raw_data.csv \
            --output logs/validation_report.txt
          cat logs/validation_report.txt

      - name: Upload raw data
        uses: actions/upload-artifact@v4
        with:
          name: raw-stock-data
          path: ${{ env.DATA_DIR }}/raw_data.csv
          retention-days: 30

  # ========== 特徴量エンジニアリング & 前処理 ==========
  preprocess:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: fetch-data

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Download raw data
        uses: actions/download-artifact@v4
        with:
          name: raw-stock-data
          path: ${{ env.DATA_DIR }}

      - name: Create directories
        run: |
          mkdir -p ${{ env.DATA_DIR }}/processed
          mkdir -p logs

      - name: Feature engineering
        run: |
          python src/feature_engineering.py \
            --input ${{ env.DATA_DIR }}/raw_data.csv \
            --output ${{ env.DATA_DIR }}/processed/features.csv \
            --log logs/features.log
          echo "✅ Features created"

      - name: Data preprocessing
        run: |
          python src/preprocessor.py \
            --input ${{ env.DATA_DIR }}/processed/features.csv \
            --output ${{ env.DATA_DIR }}/processed/train_data.csv \
            --test-output ${{ env.DATA_DIR }}/processed/test_data.csv \
            --log logs/preprocessing.log

      - name: Data statistics
        run: |
          python -c "
          import pandas as pd
          train = pd.read_csv('${{ env.DATA_DIR }}/processed/train_data.csv')
          test = pd.read_csv('${{ env.DATA_DIR }}/processed/test_data.csv')
          print(f'Train set shape: {train.shape}')
          print(f'Test set shape: {test.shape}')
          print(f'Features: {list(train.columns)}')
          "

      - name: Upload processed data
        uses: actions/upload-artifact@v4
        with:
          name: processed-stock-data
          path: ${{ env.DATA_DIR }}/processed/
          retention-days: 30

  # ========== モデル学習 ==========
  train-model:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: preprocess

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Create directories
        run: |
          mkdir -p ${{ env.MODEL_DIR }}
          mkdir -p logs

      - name: Download processed data
        uses: actions/download-artifact@v4
        with:
          name: processed-stock-data
          path: ${{ env.DATA_DIR }}/processed/

      - name: Train LSTM model
        run: |
          python src/train_lstm.py \
            --train-data ${{ env.DATA_DIR }}/processed/train_data.csv \
            --test-data ${{ env.DATA_DIR }}/processed/test_data.csv \
            --model-output ${{ env.MODEL_DIR }}/lstm_model.h5 \
            --scaler-output ${{ env.MODEL_DIR }}/scaler.pkl \
            --log logs/train_lstm.log \
            --epochs 50 \
            --batch-size 32

      - name: Train XGBoost model
        run: |
          python src/train_xgboost.py \
            --train-data ${{ env.DATA_DIR }}/processed/train_data.csv \
            --test-data ${{ env.DATA_DIR }}/processed/test_data.csv \
            --model-output ${{ env.MODEL_DIR }}/xgboost_model.pkl \
            --log logs/train_xgboost.log

      - name: Model evaluation
        run: |
          python src/evaluate_models.py \
            --lstm-model ${{ env.MODEL_DIR }}/lstm_model.h5 \
            --xgb-model ${{ env.MODEL_DIR }}/xgboost_model.pkl \
            --test-data ${{ env.DATA_DIR }}/processed/test_data.csv \
            --output logs/model_comparison.txt
          cat logs/model_comparison.txt

      - name: Upload models
        uses: actions/upload-artifact@v4
        with:
          name: trained-models
          path: ${{ env.MODEL_DIR }}/
          retention-days: 60

      - name: Upload training logs
        uses: actions/upload-artifact@v4
        with:
          name: training-logs
          path: logs/
          retention-days: 30

  # ========== 予測実行 ==========
  predict:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: train-model
    if: github.event_name == 'schedule' || github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Create directories
        run: |
          mkdir -p ${{ env.DATA_DIR }}/predictions
          mkdir -p logs

      - name: Download models
        uses: actions/download-artifact@v4
        with:
          name: trained-models
          path: ${{ env.MODEL_DIR }}/

      - name: Generate predictions
        run: |
          python src/predict.py \
            --lstm-model ${{ env.MODEL_DIR }}/lstm_model.h5 \
            --xgb-model ${{ env.MODEL_DIR }}/xgboost_model.pkl \
            --scaler ${{ env.MODEL_DIR }}/scaler.pkl \
            --us-symbols AAPL GOOGL MSFT TSLA \
            --jp-symbols 9984 6758 7203 8306 \
            --days-ahead 5 \
            --output ${{ env.DATA_DIR }}/predictions/forecast.csv \
            --log logs/predict.log

      - name: Display predictions
        run: |
          echo "=== Stock Price Forecasts ==="
          python -c "
          import pandas as pd
          df = pd.read_csv('${{ env.DATA_DIR }}/predictions/forecast.csv')
          print(df.to_string())
          "

      - name: Upload predictions
        uses: actions/upload-artifact@v4
        with:
          name: stock-predictions-${{ github.run_number }}
          path: ${{ env.DATA_DIR }}/predictions/
          retention-days: 7

  # ========== Docker ビルド ==========
  docker-build:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [build, train-model]
    if: github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Build Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          push: false
          tags: stock-predictor:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # ========== デプロイ準備 ==========
  deploy-check:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [build, train-model, predict]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Generate deployment report
        run: |
          cat > DEPLOYMENT_REPORT.md << 'EOF'
          # Stock Price Prediction Pipeline - デプロイレポート
          
          **実行日時**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          **ブランチ**: ${{ github.ref_name }}
          **コミット**: ${{ github.sha }}
          
          ## 実行結果
          - ✅ ビルド: ${{ needs.build.result }}
          - ✅ モデル学習: ${{ needs.train-model.result }}
          - ✅ 予測生成: ${{ needs.predict.result }}
          
          ## 処理内容
          1. **データ取得**: Yahoo Finance から US株（AAPL, GOOGL, MSFT, TSLA）と JP株（9984ソフトバンク, 6758ソニー, 7203トヨタ, 8306三菱UFJ）の株価取得
          2. **特徴量エンジニアリング**: 技術指標（移動平均、RSI、MACD等）生成
          3. **前処理**: 正規化、スケーリング、訓練/テスト分割
          4. **モデル学習**: LSTM と XGBoost の 2 モデルを学習
          5. **評価**: 両モデルを比較、最適なモデルを選択
          6. **予測**: 5営業日先の株価予測を生成（US + JP株）
          
          ## 出力ファイル
          - 予測結果: `predictions/forecast.csv`
          - 学習済みモデル: `models/`
          - ログ: `logs/`
          
          EOF
          cat DEPLOYMENT_REPORT.md

      - name: Upload report
        uses: actions/upload-artifact@v4
        with:
          name: deployment-report-${{ github.run_number }}
          path: DEPLOYMENT_REPORT.md
          retention-days: 7

  # ========== ステータス通知 ==========
  status:
    runs-on: ubuntu-latest
    needs: [build, fetch-data, preprocess, train-model, predict, docker-build]
    if: always()

    steps:
      - name: Pipeline status
        run: |
          echo "# Stock Price Prediction Pipeline - 実行結果"
          echo ""
          echo "| ジョブ | ステータス |"
          echo "|--------|-----------|"
          echo "| Build | ${{ needs.build.result }} |"
          echo "| Data Fetch | ${{ needs.fetch-data.result }} |"
          echo "| Preprocess | ${{ needs.preprocess.result }} |"
          echo "| Train Model | ${{ needs.train-model.result }} |"
          echo "| Predict | ${{ needs.predict.result }} |"
          echo "| Docker Build | ${{ needs.docker-build.result }} |"
          echo ""
          
          if [ "${{ needs.build.result }}" = "success" ] && [ "${{ needs.predict.result }}" = "success" ]; then
            echo "✅ パイプライン成功 - 予測が生成されました"
          else
            echo "⚠️ パイプライン失敗 - ログを確認してください"
          fi
