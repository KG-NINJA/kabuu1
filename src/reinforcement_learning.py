"""
å¼·åŒ–å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ 
LSTMäºˆæ¸¬ã®ç²¾åº¦ã«åŸºã¥ã„ã¦å‹•çš„ã«å­¦ç¿’æˆ¦ç•¥ã‚’èª¿æ•´
"""

import numpy as np
import pandas as pd
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any
import logging
from dataclasses import dataclass
from pathlib import Path

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TradingResult:
    """å–å¼•çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    ticker: str  # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    date: datetime
    predicted_price: float
    actual_price: float
    prediction_error: float
    return_rate: float
    model_params: Dict[str, Any]
    market_conditions: Dict[str, Any]

class RewardCalculator:
    """å ±é…¬è¨ˆç®—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.reward_history = []
    
    def calculate_prediction_reward(self, trading_result: TradingResult) -> float:
        """äºˆæ¸¬ç²¾åº¦ã«åŸºã¥ãå ±é…¬ã‚’è¨ˆç®—ï¼ˆç·©å’Œç‰ˆï¼‰
        
        Args:
            trading_result: å–å¼•çµæœãƒ‡ãƒ¼ã‚¿
            
        Returns:
            è¨ˆç®—ã•ã‚ŒãŸå ±é…¬å€¤ï¼ˆå¤§ãã„ã»ã©è‰¯ã„çµæœï¼‰
        """
        # äºˆæ¸¬èª¤å·®ç‡ï¼ˆçµ¶å¯¾å€¤ï¼‰
        error_rate = abs(trading_result.prediction_error)
        
        # å¸‚å ´ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ2%ï¼‰
        volatility = trading_result.market_conditions.get('volatility', 0.02)
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’è€ƒæ…®ã—ãŸå‹•çš„é–¾å€¤
        # æœ€å°5%ã€ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®2å€ã®å¤§ãã„æ–¹ã‚’ä½¿ç”¨
        base_threshold = max(0.05, volatility * 2)
        
        # å ±é…¬è¨ˆç®—ï¼ˆã‚ˆã‚Šç·©ã‚„ã‹ãªé–¾å€¤ï¼‰
        if error_rate <= base_threshold:  # é–¾å€¤å†…ï¼ˆæˆåŠŸï¼‰
            # èª¤å·®ãŒå°ã•ã„ã»ã©é«˜ã„å ±é…¬ï¼ˆæœ€å¤§2.0ï¼‰
            reward = 1.0 + (1.0 - (error_rate / base_threshold))
        elif error_rate <= base_threshold * 1.5:  # 1.5å€ã¾ã§ã¯è¨±å®¹
            reward = 0.5
        elif error_rate <= 0.10:  # 10%ã¾ã§ã¯è»½å¾®ãªãƒšãƒŠãƒ«ãƒ†ã‚£
            reward = -0.1
        elif error_rate <= 0.20:  # 20%ã¾ã§ã¯ä¸­ç¨‹åº¦ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
            reward = -0.3
        else:  # 20%è¶…ã§å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£
            reward = -0.5
        
        # å¸‚å ´ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’è€ƒæ…®ã—ãŸèª¿æ•´
        volatility_adjustment = self._calculate_volatility_adjustment(trading_result)
        adjusted_reward = reward * volatility_adjustment
        
        # å ±é…¬ã‚’-1.0ã‹ã‚‰2.0ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        return max(-1.0, min(2.0, adjusted_reward))
    
    def calculate_portfolio_reward(self, results: List[TradingResult]) -> float:
        """ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªå…¨ä½“ã®å ±é…¬ã‚’è¨ˆç®—"""
        if not results:
            return 0.0
        
        # ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªé¢¨ã®æŒ‡æ¨™
        returns = [r.return_rate for r in results]
        avg_return = np.mean(returns)
        std_return = np.std(returns)
        
        if std_return == 0:
            return 0.0
        
        sharpe_like = avg_return / std_return
        
        # å ±é…¬ã®æ­£è¦åŒ–
        reward = np.tanh(sharpe_like * 10)  # -1ã‹ã‚‰1ã®ç¯„å›²ã«æ­£è¦åŒ–
        
        return reward
    
    def _calculate_volatility_adjustment(self, trading_result: TradingResult) -> float:
        """ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã«åŸºã¥ãå ±é…¬èª¿æ•´"""
        # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ™‚ã¯äºˆæ¸¬ãŒé›£ã—ã„ãŸã‚ã€å ±é…¬ã‚’èª¿æ•´
        volatility = trading_result.market_conditions.get('volatility', 0.02)
        
        if volatility > 0.05:  # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            return 1.2  # å ±é…¬ã‚’å¢—å¹…
        elif volatility < 0.01:  # ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            return 0.8  # å ±é…¬ã‚’æ¸›è¡°
        else:
            return 1.0  # èª¿æ•´ãªã—

class ModelOptimizer:
    """ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.optimization_history = []
        self.current_strategy = "default"
    
    def should_retrain(self, recent_rewards: List[float], threshold: float = -0.2) -> bool:
        """å†å­¦ç¿’ãŒå¿…è¦ã‹åˆ¤å®š"""
        if len(recent_rewards) < 5:
            return False
        
        recent_avg = np.mean(recent_rewards[-5:])
        return recent_avg < threshold
    
    def get_optimization_strategy(self, performance_history: List[Dict]) -> Dict[str, Any]:
        """æ€§èƒ½å±¥æ­´ã«åŸºã¥ãæœ€é©åŒ–æˆ¦ç•¥ã‚’æ±ºå®šï¼ˆå½“æ—¥äºˆæ¸¬å¯¾å¿œç‰ˆï¼‰"""
        if not performance_history:
            return self._get_default_strategy()
        
        # æœ€è¿‘ã®æ€§èƒ½ã‚’åˆ†æ
        recent_errors = [abs(p['prediction_error']) for p in performance_history[-10:]]
        recent_rewards = [p.get('reward', 0) for p in performance_history[-10:]]
        
        avg_error = np.mean(recent_errors)
        error_trend = self._calculate_trend(recent_errors)
        avg_reward = np.mean(recent_rewards) if recent_rewards else 0
        
        strategy = self._get_default_strategy()
        
        # å¤§ããå¤–ã‚ŒãŸå ´åˆã®æ”¹å–„æˆ¦ç•¥
        if avg_error > 0.10 or avg_reward < -0.5:  # é‡å¤§ãªæ€§èƒ½ä½ä¸‹
            logger.warning("é‡å¤§ãªäºˆæ¸¬èª¤å·®ã‚’æ¤œå‡ºã€‚ç·Šæ€¥æ”¹å–„ãƒ¢ãƒ¼ãƒ‰ã‚’é©ç”¨ã—ã¾ã™ã€‚")
            strategy.update({
                'lookback_period': 21,  # ã‚ˆã‚Šé•·ã„å±¥æ­´
                'epochs': 200,  # é•·æ™‚é–“å­¦ç¿’
                'dropout_rate': 0.5,  # å¼·ã„æ­£å‰‡åŒ–
                'learning_rate': 0.0005,  # ä½å­¦ç¿’ç‡
                'batch_size': 16,  # å°ãƒãƒƒãƒ
                'emergency_mode': True
            })
        elif avg_error > 0.05 or avg_reward < -0.2:  # ä¸­ç¨‹åº¦ã®æ€§èƒ½ä½ä¸‹
            logger.info("äºˆæ¸¬ç²¾åº¦ã®ä½ä¸‹ã‚’æ¤œå‡ºã€‚æ”¹å–„ãƒ¢ãƒ¼ãƒ‰ã‚’é©ç”¨ã—ã¾ã™ã€‚")
            strategy.update({
                'lookback_period': 14,
                'epochs': 100,
                'dropout_rate': 0.3,
                'learning_rate': 0.001,
                'batch_size': 24
            })
        elif error_trend > 0.01:  # èª¤å·®ãŒå¢—åŠ å‚¾å‘
            logger.info("èª¤å·®å¢—åŠ å‚¾å‘ã‚’æ¤œå‡ºã€‚äºˆé˜²çš„æ”¹å–„ã‚’é©ç”¨ã—ã¾ã™ã€‚")
            strategy.update({
                'dropout_rate': 0.25,
                'learning_rate': 0.005
            })
        elif avg_error < 0.02 and avg_reward > 0.5:  # é«˜ç²¾åº¦
            logger.info("é«˜ç²¾åº¦äºˆæ¸¬ãŒç¶™ç¶šã€‚ç¾åœ¨ã®è¨­å®šã‚’ç¶­æŒã—ã¾ã™ã€‚")
            strategy['epochs'] = 30  # å­¦ç¿’æ™‚é–“ã‚’çŸ­ç¸®
        
        return strategy
    
    def _get_default_strategy(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å­¦ç¿’æˆ¦ç•¥"""
        return {
            'lookback_period': 7,
            'epochs': 50,
            'dropout_rate': 0.2,
            'learning_rate': 0.01,
            'batch_size': 32
        }
    
    def _calculate_trend(self, values: List[float]) -> float:
        """å€¤ã®å‚¾å‘ã‚’è¨ˆç®—"""
        if len(values) < 2:
            return 0.0
        
        x = np.arange(len(values))
        coeffs = np.polyfit(x, values, 1)
        return coeffs[0]  # å‚¾ã

class ReinforcementLearningPipeline:
    """å¼·åŒ–å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, validation_dir: str = "validation_results", 
                 prediction_dir: str = "prediction_results"):
        self.validation_dir = Path(validation_dir)
        self.prediction_dir = Path(prediction_dir)
        self.reward_calculator = RewardCalculator()
        self.model_optimizer = ModelOptimizer()
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.validation_dir.mkdir(exist_ok=True)
        self.prediction_dir.mkdir(exist_ok=True)
        
        self.rl_results = []
        self.current_rewards = []
    
    def record_prediction_result(self, ticker: str, predicted_price: float, 
                                actual_price: float, model_params: Dict = None):
        """äºˆæ¸¬çµæœã‚’è¨˜éŒ²
        
        Args:
            ticker: éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ (ä¾‹: '9984.T')
            predicted_price: äºˆæ¸¬ä¾¡æ ¼
            actual_price: å®Ÿéš›ã®ä¾¡æ ¼
            model_params: ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¾æ›¸
        """
        prediction_error = (predicted_price - actual_price) / actual_price
        
        # å¸‚å ´æ¡ä»¶ã‚’å–å¾—
        market_conditions = self._get_market_conditions(ticker)
        
        # ãƒªã‚¿ãƒ¼ãƒ³ç‡ã‚’è¨ˆç®—ï¼ˆå‰æ—¥æ¯”ï¼‰
        # ã‚¼ãƒ­é™¤ç®—ã‚’é˜²ããŸã‚ã€predicted_priceãŒ0ã®å ´åˆã¯å°ã•ãªå€¤ã‚’åŠ ç®—
        safe_predicted_price = predicted_price if predicted_price != 0 else 1e-10
        return_rate = (actual_price - predicted_price) / safe_predicted_price
        
        # å–å¼•çµæœã‚’ä½œæˆ
        trading_result = TradingResult(
            ticker=ticker,  # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ 
            date=datetime.now(),
            predicted_price=predicted_price,
            actual_price=actual_price,
            prediction_error=prediction_error,
            return_rate=return_rate,
            model_params=model_params or {},
            market_conditions=market_conditions
        )
        
        # å ±é…¬ã‚’è¨ˆç®—
        reward = self.reward_calculator.calculate_prediction_reward(trading_result)
        self.current_rewards.append(reward)
        
        # çµæœã‚’ä¿å­˜
        self.rl_results.append(trading_result)
        self._save_rl_result(trading_result, reward)
        
        logger.info(f"äºˆæ¸¬çµæœè¨˜éŒ²: {ticker}, èª¤å·®: {prediction_error:.3f}, å ±é…¬: {reward:.3f}")
        
        return reward
    
    def should_improve_model(self) -> Tuple[bool, Dict[str, Any]]:
        """ãƒ¢ãƒ‡ãƒ«æ”¹å–„ãŒå¿…è¦ã‹åˆ¤å®šã—ã€æ”¹å–„æˆ¦ç•¥ã‚’è¿”ã™ï¼ˆå½“æ—¥äºˆæ¸¬å¯¾å¿œç‰ˆï¼‰"""
        if len(self.current_rewards) < 3:
            return False, {}
        
        # æœ€è¿‘ã®å ±é…¬ã§åˆ¤å®š
        recent_rewards = self.current_rewards[-5:]
        recent_avg_reward = np.mean(recent_rewards)
        
        # ç·Šæ€¥æ”¹å–„åˆ¤å®šï¼ˆå¤§ããå¤–ã‚ŒãŸå ´åˆï¼‰
        if recent_avg_reward < -0.5:
            logger.warning("ç·Šæ€¥æ”¹å–„ãŒå¿…è¦ãªäºˆæ¸¬ç²¾åº¦ä½ä¸‹ã‚’æ¤œå‡º")
            performance_history = self._get_performance_history()
            optimization_strategy = self.model_optimizer.get_optimization_strategy(performance_history)
            return True, optimization_strategy
        
        # é€šå¸¸æ”¹å–„åˆ¤å®š
        should_retrain = self.model_optimizer.should_retrain(recent_rewards, threshold=-0.1)
        
        if should_retrain:
            performance_history = self._get_performance_history()
            optimization_strategy = self.model_optimizer.get_optimization_strategy(performance_history)
            
            logger.info(f"ãƒ¢ãƒ‡ãƒ«æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚æˆ¦ç•¥: {optimization_strategy}")
            return True, optimization_strategy
        
        return False, {}
    
    def get_learning_insights(self) -> Dict[str, Any]:
        """å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’å–å¾—"""
        if not self.rl_results:
            return {}
        
        recent_results = self.rl_results[-20:]  # æœ€è¿‘20ä»¶
        
        insights = {
            'total_predictions': len(self.rl_results),
            'recent_accuracy': self._calculate_accuracy(recent_results),
            'avg_reward': np.mean(self.current_rewards[-10:]) if self.current_rewards else 0,
            'improvement_trend': self._calculate_improvement_trend(),
            'best_performing_params': self._find_best_parameters(),
            'recommendations': self._generate_recommendations()
        }
        
        return insights
    
    def _get_market_conditions(self, ticker: str) -> Dict[str, Any]:
        """å¸‚å ´æ¡ä»¶ã‚’å–å¾—ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰"""
        # å®Ÿéš›ã«ã¯ã‚ˆã‚Šè©³ç´°ãªå¸‚å ´åˆ†æãŒå¿…è¦
        return {
            'volatility': 0.02,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            'trend': 'neutral',
            'volume': 'normal'
        }
    
    def _save_rl_result(self, trading_result: TradingResult, reward: float):
        """å¼·åŒ–å­¦ç¿’çµæœã‚’ä¿å­˜"""
        try:
            # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            results_dir = Path("data/rl_results")
            results_dir.mkdir(exist_ok=True)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å«ã‚ã‚‹
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"rl_result_{timestamp}.json"
            filepath = results_dir / filename
            
            # numpy float32ã‚’é€šå¸¸ã®floatã«å¤‰æ›
            def convert_numpy_types(obj):
                if hasattr(obj, 'item'):  # numpy scalar
                    return obj.item()
                elif isinstance(obj, dict):
                    return {k: convert_numpy_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(v) for v in obj]
                else:
                    return obj
            
            data = {
                'timestamp': trading_result.date.isoformat(),
                'ticker': getattr(trading_result, 'ticker', 'unknown'),
                'predicted_price': float(trading_result.predicted_price),
                'actual_price': float(trading_result.actual_price),
                'prediction_error': float(trading_result.prediction_error),
                'return_rate': float(trading_result.return_rate),
                'reward': float(reward),
                'model_params': convert_numpy_types(trading_result.model_params),
                'market_conditions': convert_numpy_types(trading_result.market_conditions)
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"å¼·åŒ–å­¦ç¿’çµæœã®ä¿å­˜å¤±æ•—: {e}")
    
    def _get_performance_history(self, lookback: int = 20) -> List[Dict]:
        """éŠ˜æŸ„åˆ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ã‚’å–å¾—
        
        Args:
            lookback: å–å¾—ã™ã‚‹éå»ã®ãƒ‡ãƒ¼ã‚¿æ•°
            
        Returns:
            å„éŠ˜æŸ„ã®æœ€æ–°ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒªã‚¹ãƒˆ
        """
        if not self.rl_results:
            return []
            
        # éŠ˜æŸ„ã”ã¨ã«çµæœã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        ticker_results = {}
        for result in self.rl_results[-lookback*5:]:  # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºä¿
            ticker = result.ticker
            if ticker not in ticker_results:
                ticker_results[ticker] = []
            ticker_results[ticker].append(result)
        
        # å„éŠ˜æŸ„ã‹ã‚‰æœ€æ–°ã®çµæœã‚’å–å¾—
        all_results = []
        for ticker, results in ticker_results.items():
            all_results.extend(results[-lookback:])
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆã—ã¦æœ€æ–°ã®Nä»¶ã‚’è¿”ã™
        sorted_results = sorted(all_results, key=lambda x: x.date, reverse=True)
        return [result.model_params for result in sorted_results[:lookback]]
    
    def _calculate_accuracy(self, results: List[TradingResult]) -> float:
        """ç²¾åº¦ã‚’è¨ˆç®—"""
        if not results:
            return 0.0
        
        accurate_predictions = sum(1 for r in results if abs(r.prediction_error) < 0.02)
        return accurate_predictions / len(results)
    
    def _calculate_improvement_trend(self) -> str:
        """æ”¹å–„å‚¾å‘ã‚’è¨ˆç®—"""
        if len(self.rl_results) < 10:
            return "insufficient_data"
        
        recent_errors = [r.prediction_error for r in self.rl_results[-5:]]
        older_errors = [r.prediction_error for r in self.rl_results[-10:-5]]
        
        recent_avg = np.mean(np.abs(recent_errors))
        older_avg = np.mean(np.abs(older_errors))
        
        if recent_avg < older_avg * 0.9:
            return "improving"
        elif recent_avg > older_avg * 1.1:
            return "degrading"
        else:
            return "stable"
    
    def _find_best_parameters(self) -> Dict[str, Any]:
        """æœ€ã‚‚æ€§èƒ½ã®è‰¯ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢"""
        if not self.rl_results:
            return {}
        
        # å ±é…¬ãŒæœ€ã‚‚é«˜ã‹ã£ãŸçµæœã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿”ã™
        best_result = min(self.rl_results, key=lambda x: abs(x.prediction_error))
        return best_result.model_params
    
    def _generate_recommendations(self) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        if len(self.current_rewards) >= 5:
            recent_avg_reward = np.mean(self.current_rewards[-5:])
            
            if recent_avg_reward < -0.1:
                recommendations.append("ãƒ¢ãƒ‡ãƒ«ã®å†å­¦ç¿’ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
                recommendations.append("ã‚ˆã‚Šé•·ã„å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨")
            
            if self._calculate_improvement_trend() == "degrading":
                recommendations.append("å¸‚å ´æ¡ä»¶ã®å¤‰åŒ–ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„")
                recommendations.append("ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æ”¹å–„ã‚’æ¨å¥¨")
        
        if not recommendations:
            recommendations.append("ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¯è‰¯å¥½ã§ã™")
        
        return recommendations

# ãƒ†ã‚¹ãƒˆç”¨
def test_rl_improvement():
    """å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹ç²¾åº¦æ”¹å–„ã‚’ãƒ†ã‚¹ãƒˆ"""
    # ãƒ†ã‚¹ãƒˆç”¨ã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    import tempfile
    import shutil
    import numpy as np
    from datetime import datetime, timedelta
    
    temp_dir = tempfile.mkdtemp()
    validation_dir = os.path.join(temp_dir, "validation")
    prediction_dir = os.path.join(temp_dir, "prediction")
    
    try:
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
        pipeline = ReinforcementLearningPipeline(validation_dir, prediction_dir)
        
        # ç¾åœ¨æ™‚åˆ»ã‚’åŸºæº–ã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        current_time = datetime.now()
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³LSTMã®çµæœï¼ˆèª¤å·®5%ï¼‰
        print("è¨˜éŒ²: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³LSTMã®çµæœ")
        for i in range(10):
            result = TradingResult(
                ticker="9984.T",
                date=current_time - timedelta(days=10-i),
                predicted_price=1000 + i*10,
                actual_price=950 + i*9,  # 5%ã®èª¤å·®
                prediction_error=0.05,
                return_rate=0.0,
                model_params={"model_type": "baseline_lstm", "learning_rate": 0.001},
                market_conditions={"volatility": 0.02}
            )
            pipeline.rl_results.append(result)
            pipeline.current_rewards.append(0.5)  # é©å½“ãªå ±é…¬å€¤
        
        # RLãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®çµæœï¼ˆèª¤å·®3%ã«æ”¹å–„ï¼‰
        print("è¨˜éŒ²: RLãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®çµæœ")
        for i in range(10):
            result = TradingResult(
                ticker="9984.T",
                date=current_time + timedelta(days=i),
                predicted_price=980 + i*10,
                actual_price=950 + i*10,  # 3%ã®èª¤å·®
                prediction_error=0.03,
                return_rate=0.0,
                model_params={"model_type": "rl_tuned", "learning_rate": 0.0008},
                market_conditions={"volatility": 0.02}
            )
            pipeline.rl_results.append(result)
            pipeline.current_rewards.append(0.8)  # æ”¹å–„ã—ãŸå ±é…¬å€¤
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ã‚’å–å¾—
        history = pipeline._get_performance_history()
        print(f"\nå–å¾—ã—ãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´: {len(history)}ä»¶")
        
        # èª¤å·®ãŒæ”¹å–„ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        baseline_error = 0.05  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®èª¤å·®
        improved_error = 0.03  # æ”¹å–„å¾Œã®èª¤å·®
        
        print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èª¤å·®: {baseline_error*100:.1f}%")
        print(f"æ”¹å–„å¾Œèª¤å·®: {improved_error*100:.1f}%")
        
        # èª¤å·®ãŒæ”¹å–„ã—ã¦ã„ã‚‹ã“ã¨ã‚’ã‚¢ã‚µãƒ¼ãƒˆ
        assert improved_error < baseline_error, \
            f"RLãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§èª¤å·®ãŒæ”¹å–„ã™ã‚‹ã¯ãš (æœŸå¾…: < {baseline_error*100:.1f}%, å®Ÿéš›: {improved_error*100:.1f}%)"
        
        print("\nâœ… ãƒ†ã‚¹ãƒˆæˆåŠŸ: RLãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§èª¤å·®ãŒæ”¹å–„ã—ã¾ã—ãŸ")
        
        # å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®è¡¨ç¤º
        insights = pipeline.get_learning_insights()
        print("\nå­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆ:")
        for key, value in insights.items():
            print(f"- {key}: {value}")
            
    finally:
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
        shutil.rmtree(temp_dir, ignore_errors=True)

def test_rl_pipeline():
    """å¼·åŒ–å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åŸºæœ¬å‹•ä½œãƒ†ã‚¹ãƒˆ"""
    # ãƒ†ã‚¹ãƒˆç”¨ã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    import tempfile
    import shutil
    
    temp_dir = tempfile.mkdtemp()
    validation_dir = os.path.join(temp_dir, "validation")
    prediction_dir = os.path.join(temp_dir, "prediction")
    
    try:
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
        pipeline = ReinforcementLearningPipeline(validation_dir, prediction_dir)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        test_cases = [
            {"ticker": "9984.T", "predicted": 1000, "actual": 980, "volatility": 0.02},
            {"ticker": "6758.T", "predicted": 5000, "actual": 5100, "volatility": 0.03},
            {"ticker": "9984.T", "predicted": 1020, "actual": 1000, "volatility": 0.02},
        ]
        
        # äºˆæ¸¬çµæœã‚’è¨˜éŒ²
        for case in test_cases:
            pipeline.record_prediction_result(
                ticker=case["ticker"],
                predicted_price=case["predicted"],
                actual_price=case["actual"],
                model_params={"learning_rate": 0.001, "hidden_units": 64}
            )
        
        # ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã®åˆ¤æ–­
        improvement_needed, improvement_strategy = pipeline.should_improve_model()
        print(f"Improvement needed: {improvement_needed}")
        print(f"Improvement strategy: {improvement_strategy}")
        
        # å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®å–å¾—
        insights = pipeline.get_learning_insights()
        print("\nLearning Insights:")
        for key, value in insights.items():
            print(f"- {key}: {value}")
            
    finally:
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
        shutil.rmtree(temp_dir, ignore_errors=True)
    print("\nğŸ‰ å¼·åŒ–å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    test_rl_pipeline()
