"""
æ¯æ—¥å¼·ããªã‚‹ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…
å ±é…¬ã«åŸºã¥ã„ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•çš„ã«èª¿æ•´
"""

from pathlib import Path
from datetime import datetime
import json
import logging
from typing import Dict, Any, Tuple, Optional

logger = logging.getLogger(__name__)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å®šæ•°
TARGET_SYMBOL = "NVDA"


class AdaptiveNVDALearner:
    """å ±é…¬ãƒ™ãƒ¼ã‚¹ã®é©å¿œçš„å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, log_path: Path = Path("reinforcement_learning.log")):
        self.log_path = log_path
        self.rewards_history = []
        self.params_history = []
        self.load_history()
    
    def load_history(self) -> None:
        """å ±é…¬å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€"""
        if not self.log_path.exists():
            return
        
        try:
            with open(self.log_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                for line in lines[1:]:  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    parts = line.strip().split(',')
                    if len(parts) >= 4:
                        try:
                            reward = float(parts[3])
                            self.rewards_history.append(reward)
                        except ValueError:
                            pass
        except Exception as e:
            logger.warning(f"å ±é…¬å±¥æ­´ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    
    def get_average_reward(self, window: int = 5) -> float:
        """ç›´è¿‘Næ—¥ã®å¹³å‡å ±é…¬ã‚’è¨ˆç®—"""
        if not self.rewards_history:
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        recent = self.rewards_history[-window:]
        return sum(recent) / len(recent)
    
    def should_improve_model(self) -> Tuple[bool, Dict[str, Any]]:
        """ãƒ¢ãƒ‡ãƒ«æ”¹å–„ãŒå¿…è¦ã‹åˆ¤å®šã—ã€æˆ¦ç•¥ã‚’ææ¡ˆ"""
        
        if len(self.rewards_history) < 3:
            return False, {}  # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã‚‹
        
        avg_reward = self.get_average_reward(window=5)
        recent_trend = self.rewards_history[-1] - self.rewards_history[-2] if len(self.rewards_history) > 1 else 0
        
        strategy = {
            "timestamp": datetime.now().isoformat(),
            "average_reward": avg_reward,
            "recent_trend": recent_trend,
            "adjustments": {}
        }
        
        # é«˜æ€§èƒ½ï¼šç¾åœ¨ã®æˆ¦ç•¥ã‚’ä¿æŒ
        if avg_reward > 0.99:
            strategy["adjustments"] = {
                "action": "maintain",
                "reason": f"é«˜æ€§èƒ½ã‚’ç¶­æŒï¼ˆå¹³å‡å ±é…¬: {avg_reward:.4f}ï¼‰"
            }
            return False, strategy
        
        # æ”¹å–„å‚¾å‘ï¼šå­¦ç¿’ç‡ã‚’å°‘ã—ä¸Šã’ã¦ç©æ¥µçš„ã«å­¦ç¿’
        elif avg_reward > 0.95 and recent_trend > 0:
            strategy["adjustments"] = {
                "action": "increase_learning",
                "learning_rate": 0.002,  # 0.001 â†’ 0.002
                "reason": f"æ”¹å–„å‚¾å‘ãŒè¦‹ã‚‰ã‚Œã¾ã™ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰: {recent_trend:.4f}ï¼‰"
            }
            return True, strategy
        
        # ä½æ€§èƒ½ï¼šãƒ‡ãƒ¼ã‚¿æœŸé–“ã‚’æ‹¡å¤§ã—ã¦å†å­¦ç¿’
        elif avg_reward < 0.95:
            strategy["adjustments"] = {
                "action": "retrain_with_more_data",
                "lookback_period": 180,  # 90 â†’ 180 æ—¥
                "learning_rate": 0.001,
                "reason": f"æ€§èƒ½ä½ä¸‹ã‚’æ¤œå‡ºï¼ˆå¹³å‡å ±é…¬: {avg_reward:.4f}ï¼‰"
            }
            return True, strategy
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šæ§˜å­ã‚’è¦‹ã‚‹
        return False, strategy
    
    def get_learning_report(self) -> Dict[str, Any]:
        """å­¦ç¿’é€²æ—ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        
        if not self.rewards_history:
            return {"status": "No data"}
        
        report = {
            "total_predictions": len(self.rewards_history),
            "latest_reward": round(self.rewards_history[-1], 6),
            "average_reward_all": round(sum(self.rewards_history) / len(self.rewards_history), 6),
            "average_reward_7day": round(sum(self.rewards_history[-7:]) / min(7, len(self.rewards_history)), 6),
            "best_reward": round(max(self.rewards_history), 6),
            "worst_reward": round(min(self.rewards_history), 6),
            "trend": "improving" if (self.rewards_history[-1] > self.rewards_history[-2] if len(self.rewards_history) > 1 else False) else "stable/declining",
        }
        
        return report


class NvdaReinforcementHub:
    """å¼·åŒ–å­¦ç¿’ãƒãƒ– - äºˆæ¸¬çµæœã‚’è¨˜éŒ²ã—ã€æ”¹å–„æˆ¦ç•¥ã‚’ææ¡ˆ"""
    
    def __init__(self, base_dir: Optional[str] = None, reward_threshold: float = 0.0):
        """
        åˆæœŸåŒ–
        
        Args:
            base_dir: ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: nvda_learningï¼‰
            reward_threshold: å ±é…¬é–¾å€¤
        """
        self.base_dir = Path(base_dir or "nvda_learning")
        self.validation_dir = self.base_dir / "validation"
        self.prediction_dir = self.base_dir / "predictions"
        self.reward_threshold = reward_threshold
        self.log_path = Path("reinforcement_learning.log")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        for d in [self.validation_dir, self.prediction_dir]:
            d.mkdir(parents=True, exist_ok=True)
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã‘ã‚Œã°ä½œæˆ
        if not self.log_path.exists():
            with open(self.log_path, 'w', encoding='utf-8') as f:
                f.write("ticker,predicted,actual,reward\n")
        
        self.learner = AdaptiveNVDALearner(self.log_path)
        logger.info(f"NvdaReinforcementHub ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ (base_dir: {self.base_dir})")
    
    def record_outcome(
        self, 
        ticker: str, 
        predicted_price: float, 
        actual_price: float, 
        model_params: Optional[Dict[str, Any]] = None
    ) -> float:
        """
        äºˆæ¸¬çµæœã‚’è¨˜éŒ²ã—ã€å ±é…¬ã‚’è¨ˆç®—
        
        Args:
            ticker: éŠ˜æŸ„ï¼ˆä¾‹: NVDAï¼‰
            predicted_price: äºˆæ¸¬ä¾¡æ ¼
            actual_price: å®Ÿéš›ã®ä¾¡æ ¼
            model_params: ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            è¨ˆç®—ã•ã‚ŒãŸå ±é…¬ã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰
        """
        try:
            # å ±é…¬ã‚’è¨ˆç®—ï¼ˆèª¤å·®ãŒå°ã•ã„ã»ã©å ±é…¬ãŒå¤§ãã„ï¼‰
            if actual_price == 0:
                reward = 0.5
            else:
                error = abs(predicted_price - actual_price) / actual_price
                reward = max(0.0, min(1.0, 1.0 - error))  # 0.0-1.0 ã®ç¯„å›²ã«æ­£è¦åŒ–
            
            # ãƒ­ã‚°ã«è¨˜éŒ²
            with open(self.log_path, 'a', encoding='utf-8') as f:
                f.write(f"{ticker},{predicted_price:.4f},{actual_price:.4f},{reward:.6f}\n")
            
            logger.info(
                f"çµæœã‚’è¨˜éŒ²ã—ã¾ã—ãŸ: {ticker} - äºˆæ¸¬: ${predicted_price:.2f}, "
                f"å®Ÿç¸¾: ${actual_price:.2f}, å ±é…¬: {reward:.6f}"
            )
            
            return reward
        
        except Exception as e:
            logger.error(f"çµæœã®è¨˜éŒ²ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return 0.5
    
    def should_improve(self) -> Tuple[bool, Dict[str, Any]]:
        """
        ãƒ¢ãƒ‡ãƒ«æ”¹å–„ãŒå¿…è¦ã‹åˆ¤å®š
        
        Returns:
            (æ”¹å–„ãŒå¿…è¦ã‹, æ”¹å–„æˆ¦ç•¥)
        """
        try:
            return self.learner.should_improve_model()
        except Exception as e:
            logger.error(f"æ”¹å–„åˆ¤å®šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False, {}
    
    def get_learning_insights(self) -> Dict[str, Any]:
        """
        å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’å–å¾—
        
        Returns:
            å­¦ç¿’é€²æ—ãƒ¬ãƒãƒ¼ãƒˆ
        """
        try:
            return self.learner.get_learning_report()
        except Exception as e:
            logger.error(f"å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return {"status": "Error"}


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    
    # AdaptiveNVDALearner ã®ãƒ†ã‚¹ãƒˆ
    print("=" * 60)
    print("AdaptiveNVDALearner ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    learner = AdaptiveNVDALearner()
    
    # å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆ
    report = learner.get_learning_report()
    print("\nğŸ“Š å­¦ç¿’é€²æ—ãƒ¬ãƒãƒ¼ãƒˆ:")
    for key, value in report.items():
        print(f"  {key}: {value}")
    
    # æ”¹å–„ãŒå¿…è¦ã‹åˆ¤å®š
    should_improve, strategy = learner.should_improve_model()
    print(f"\nğŸ”§ æ”¹å–„ãŒå¿…è¦: {should_improve}")
    print(f"ğŸ’¡ æ¨å¥¨æˆ¦ç•¥: {json.dumps(strategy, indent=2, ensure_ascii=False)}")
    
    # NvdaReinforcementHub ã®ãƒ†ã‚¹ãƒˆ
    print("\n" + "=" * 60)
    print("NvdaReinforcementHub ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    hub = NvdaReinforcementHub()
    
    # çµæœã‚’è¨˜éŒ²
    print("\nğŸ“ çµæœã‚’è¨˜éŒ²:")
    reward = hub.record_outcome(
        ticker=TARGET_SYMBOL,
        predicted_price=186.93,
        actual_price=186.86,
        model_params={"learning_rate": 0.001}
    )
    print(f"  å ±é…¬: {reward:.6f}")
    
    # æ”¹å–„åˆ¤å®š
    print("\nğŸ”§ æ”¹å–„åˆ¤å®š:")
    should_improve, strategy = hub.should_improve()
    print(f"  æ”¹å–„ãŒå¿…è¦: {should_improve}")
    
    # å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
    print("\nğŸ“ˆ å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆ:")
    insights = hub.get_learning_insights()
    for key, value in insights.items():
        print(f"  {key}: {value}")
